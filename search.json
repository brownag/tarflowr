[{"path":"http://humus.rocks/tarflowr/LICENSE.html","id":null,"dir":"","previous_headings":"","what":"MIT License","title":"MIT License","text":"Copyright (c) 2025 tarflowr authors Permission hereby granted, free charge, person obtaining copy software associated documentation files (“Software”), deal Software without restriction, including without limitation rights use, copy, modify, merge, publish, distribute, sublicense, /sell copies Software, permit persons Software furnished , subject following conditions: copyright notice permission notice shall included copies substantial portions Software. SOFTWARE PROVIDED “”, WITHOUT WARRANTY KIND, EXPRESS IMPLIED, INCLUDING LIMITED WARRANTIES MERCHANTABILITY, FITNESS PARTICULAR PURPOSE NONINFRINGEMENT. EVENT SHALL AUTHORS COPYRIGHT HOLDERS LIABLE CLAIM, DAMAGES LIABILITY, WHETHER ACTION CONTRACT, TORT OTHERWISE, ARISING , CONNECTION SOFTWARE USE DEALINGS SOFTWARE.","code":""},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"introduction","dir":"Articles","previous_headings":"","what":"Introduction","title":"Orchestration of Targets Pipelines with tarflowr","text":"vignette demonstrates use tarflowr manage complex, hierarchical workflows. pattern common scientific research primary analysis pipeline needs run multiple independent datasets (e.g., different study sites, simulation parameters, patient cohorts). simulate soil science study five distinct study sites. goal model relationship soil clay content elevation site aggregate models understand overall trends variability. workflow involve two layers orchestration: Sub-Projects: five sites, use tarflowr_run() create independent project. project calculate depth-weighted average clay content 10 soil profiles fit parabolic linear model clay ~ elevation. Meta-orchestrator: use second, top-level tarflowr_run() call uses new helpers (tarflowr_project() tarflowr_run_subproject()). meta-pipeline execute five sub-projects parallel combine resulting models final summary table.","code":""},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"setup-and-helper-functions","dir":"Articles","previous_headings":"","what":"1. Setup and Helper Functions","title":"Orchestration of Targets Pipelines with tarflowr","text":"First, load necessary packages define functions ’ll need analysis. function create_site_data() creates realistic-looking soil dataset single site. function calculates depth weighted average clay content single pedon’s data","code":"# Load libraries library(tarflowr) library(targets) library(dplyr) library(tidyr) library(purrr) library(broom) create_site_data <- function(site_id) {     set.seed(which(letters == tolower(site_id))) # for reproducibility          # Site-specific elevation range and model parameters     elev_base <- runif(1, 100, 500)     elev_range <- runif(1, 50, 200)          # Parabolic relationship parameters     # y = a(x - h)^2 + k     h <- elev_base + elev_range / 2 # vertex x-value (mid-elevation)     k <- runif(1, 15, 25)           # vertex y-value (min clay)     a <- runif(1, 0.001, 0.005)     # parabola steepness          pedons <- tibble::tibble(         site_id = site_id,         pedon_id = paste0(site_id, \"_\", 1:10),         elevation = elev_base + runif(10) * elev_range     ) |>         dplyr::mutate(base_clay = a * (elevation - h)^2 + k + rnorm(10, 0, 2))          # Generate horizon data for each pedon     purrr::map_dfr(1:nrow(pedons), function(i) {         p <- pedons[i, ]         depths <- c(0, 15, 30, 60, 100)                  # Randomly make some profiles shallow         if (runif(1) < 0.2) {             depths <- c(0, 15, runif(1, 20, 49))         }                  horizons <- tibble::tibble(             site_id = p$site_id,             pedon_id = p$pedon_id,             elevation = p$elevation,             hzn_top = head(depths, -1),             hzn_bottom = tail(depths, -1),             hzn_mid = (hzn_top + hzn_bottom) / 2         ) |>             dplyr::mutate(                 # Add random argillic horizon effect                 clay_pct = p$base_clay + (hzn_mid * runif(1, 0.05, 0.2)) + rnorm(n(), 0, 1.5),                 clay_pct = pmax(5, clay_pct) # ensure clay is not negative             )         return(horizons)     }) } calculate_dwa_clay <- function(pedon_data, target_depth = 50) {          dwa_data <- pedon_data |>         dplyr::filter(hzn_top < target_depth) |>         dplyr::mutate(             hzn_bottom_clipped = pmin(hzn_bottom, target_depth),             thickness = hzn_bottom_clipped - hzn_top         ) |>         dplyr::filter(thickness > 0)          if (nrow(dwa_data) == 0) {         return(tibble(             pedon_id = unique(pedon_data$pedon_id),             elevation = unique(pedon_data$elevation),             dwa_clay = NA_real_         ))     }          # Calculate DWA     dwa_result <- dwa_data |>         dplyr::summarise(dwa_clay = sum(clay_pct * thickness) / sum(thickness))          tibble::tibble(         pedon_id = unique(pedon_data$pedon_id),         elevation = unique(pedon_data$elevation),         dwa_clay = dwa_result$dwa_clay     ) }  fit_clay_elevation_model <- function(dwa_clay_list) {     site_data <- dplyr::bind_rows(dwa_clay_list) |>         dplyr::filter(!is.na(dwa_clay))          model <- lm(dwa_clay ~ elevation + I(elevation^2), data = site_data)          list(list(         tidy = broom::tidy(model),         glance = broom::glance(model)     )) }"},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"generate-and-run-the-individual-site-sub-projects","dir":"Articles","previous_headings":"","what":"2. Generate and Run the Individual Site Sub-Projects","title":"Orchestration of Targets Pipelines with tarflowr","text":"Now, loop desired sites (-E), generate data , run tarflowr project one. create five independent, cached, reproducible analyses.","code":"site_ids <- c(\"A\", \"B\", \"C\", \"D\", \"E\") sub_project_dirs <- list()  for (site in site_ids) {     project_path <- file.path(tempdir(), paste0(\"site_\", site))     sub_project_dirs[[site]] <- project_path          site_data <- create_site_data(site)     site_work_units <- split(site_data, site_data$pedon_id)          tarflowr_run(         work_units = site_work_units,         process_func = calculate_dwa_clay,         combine_func = fit_clay_elevation_model,         project_dir = project_path,         result_target_name = \"site_model_summary\",         packages = c(\"dplyr\", \"tidyr\", \"broom\"),         callr_function = callr::r_bg,         workers = 2 # Use 2 workers for the inner loop     ) }"},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"orchestrate-sub-projects-with-a-hierarchical-workflow","dir":"Articles","previous_headings":"","what":"3. Orchestrate Sub-Projects with a Hierarchical Workflow","title":"Orchestration of Targets Pipelines with tarflowr","text":"five sub-projects completed, can now use hierarchical workflow helpers run single meta-pipeline. demonstrates re-run update sites parallel.","code":"hierarchical_work_units <- purrr::map(sub_project_dirs, ~ {     tarflowr_project(         project_dir = .x,         result_target = site_model_summary      ) })  summarize_all_models <- function(model_summary_list) {     model_summary_list <- do.call('c', model_summary_list)     s <- do.call('rbind', lapply(model_summary_list, function(x) x$glance))     cs <- do.call('rbind', lapply(model_summary_list, function(x) x$tidy)) |>          dplyr::group_by(term) |>         dplyr::summarize(             mean_estimate = mean(estimate),             sd_estimate = sd(estimate),             .groups = \"drop\"         )          list(         coef_summary = cs,          site_stats = s     ) }  meta_results <- tarflowr_run(     work_units = hierarchical_work_units,     process_func = tarflowr_run_subproject,     combine_func = summarize_all_models,     project_dir = file.path(\"_meta_analysis\"),     workers = 2 # Run 2 sub-projects in parallel )"},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"final-results","dir":"Articles","previous_headings":"","what":"4. Final Results","title":"Orchestration of Targets Pipelines with tarflowr","text":"meta-orchestrator combined results five sites. can now view final summary tables.","code":""},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"site-specific-model-fit-statistics","dir":"Articles","previous_headings":"4. Final Results","what":"Site-Specific Model Fit Statistics","title":"Orchestration of Targets Pipelines with tarflowr","text":"table shows R-squared RMSE parabolic model individual site.","code":""},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"overall-model-parameter-summary","dir":"Articles","previous_headings":"4. Final Results","what":"Overall Model Parameter Summary","title":"Orchestration of Targets Pipelines with tarflowr","text":"table shows mean standard deviation model coefficients (intercept, elevation, elevation-squared) across five sites, giving us idea overall trend variability.","code":""},{"path":"http://humus.rocks/tarflowr/articles/orchestration.html","id":"conclusion","dir":"Articles","previous_headings":"","what":"Conclusion","title":"Orchestration of Targets Pipelines with tarflowr","text":"vignette demonstrates tarflowr’s hierarchical workflow capabilities. able : Define complex, multi-step analysis single study site. Use tarflowr create run analysis independently five sites. Use tarflowr’s meta-orchestration helpers (tarflowr_project tarflowr_run_subproject) run five site-level pipelines single, parallelized workflow. Aggregate results independent pipeline final, comprehensive summary. pattern provides scalable, reproducible, organized way manage large-scale research projects.","code":""},{"path":"http://humus.rocks/tarflowr/authors.html","id":null,"dir":"","previous_headings":"","what":"Authors","title":"Authors and Citation","text":"Andrew Brown. Author, maintainer.","code":""},{"path":"http://humus.rocks/tarflowr/authors.html","id":"citation","dir":"","previous_headings":"","what":"Citation","title":"Authors and Citation","text":"Brown (2025). tarflowr: High-level Orchestration 'targets' Pipelines. R package version 0.0.1, https://github.com/brownag/tarflowr/.","code":"@Manual{,   title = {tarflowr: High-level Orchestration of 'targets' Pipelines},   author = {Andrew Brown},   year = {2025},   note = {R package version 0.0.1},   url = {https://github.com/brownag/tarflowr/}, }"},{"path":"http://humus.rocks/tarflowr/index.html","id":"tarflowr","dir":"","previous_headings":"","what":"High-level Orchestration of targets Pipelines","title":"High-level Orchestration of targets Pipelines","text":"[!NOTE] package experimental. internals hacky subject change. proof concept: kicking tires system can float level single ‘targets’ pipeline. goal {tarflowr} (say: “tar flower”) provide simple, high-level interface creating executing ‘targets’ pipelines, users can focus specific analysis, pipeline orchestration. philosophy based concept arbitrary “work units.” {tarflowr} workflow {targets} pipeline, work units within workflow can {targets} pipelines : either generated {tarflowr} custom-made. abstraction allows one develop test sub-project independently, use {tarflowr} “orchestrator” run combine sub-projects scale. tarflowr_run() function serves high-level interface ‘targets’ ‘crew’ packages. programmatically generates _targets.R pipeline file based user-provided inputs, runs pipeline, returns final result. designed abstract complexity setting ‘targets’-based workflow common “map-reduce” “split-apply-combine” tasks.","code":""},{"path":"http://humus.rocks/tarflowr/index.html","id":"installation","dir":"","previous_headings":"","what":"Installation","title":"High-level Orchestration of targets Pipelines","text":"can install development version tarflowr like :","code":"if (!require(\"remotes\")) install.packages(\"remotes\") remotes::install_github(\"brownag/tarflowr\")"},{"path":"http://humus.rocks/tarflowr/index.html","id":"example","dir":"","previous_headings":"","what":"Example","title":"High-level Orchestration of targets Pipelines","text":"basic example calculating sum squares parallel:","code":"library(tarflowr)  PROJECT_DIR <- file.path(tempdir(), \"_my_first_project\")  # define the work: a list of numbers my_work <- as.list(1:10)  # define the processing function to work on one item square_a_number <- function(x) {   Sys.sleep(1*1)   return(x^2) }  # define the combine function for the list of results sum_all_results <- function(results_list) {   sum(unlist(results_list)) }  # run the workflow final_sum <- tarflowr_run(   work_units = my_work,   process_func = square_a_number,   combine_func = sum_all_results,   project_dir = PROJECT_DIR,   workers = 4 ) #> ℹ Creating project directory: '/tmp/RtmppjxsU0/_my_first_project' #>  #> ── Starting tarflowr workflow ────────────────────────────────────────────────── #> • Project directory: '/tmp/RtmppjxsU0/_my_first_project' #> • Number of work units: 10 #> • Parallel workers: 4 #> ℹ Executing targets pipeline... #> + user_functions_file dispatched #> ✔ user_functions_file completed [1ms, 148 B] #> + work_units_file dispatched #> ✔ work_units_file completed [0ms, 137 B] #> + user_functions dispatched #> ✔ user_functions completed [0ms, 150 B] #> + project_work_units dispatched #> ✔ project_work_units completed [0ms, 137 B] #> + work_seq dispatched #> ✔ work_seq completed [0ms, 99 B] #> + processed_unit declared [10 branches] #> ✔ processed_unit completed [10s, 514 B] #> + my_first_project_result dispatched #> ✔ my_first_project_result completed [0ms, 53 B] #> ✔ ended pipeline [10.4s, 16 completed, 0 skipped]  # result is 385 final_sum #> [1] 385  # see \"_my_first_project\" for _targets.R file and _targets/ cache.  # when re-run, we get the result from the cache: cached_sum <- tarflowr_run(   work_units = my_work,   process_func = square_a_number,   combine_func = sum_all_results,   project_dir = PROJECT_DIR,   workers = 4, ) #>  #> ── Starting tarflowr workflow ────────────────────────────────────────────────── #> • Project directory: '/tmp/RtmppjxsU0/_my_first_project' #> • Number of work units: 10 #> • Parallel workers: 4 #> ℹ Executing targets pipeline... #> + user_functions dispatched #> ✔ user_functions completed [1ms, 150 B] #> + processed_unit declared [10 branches] #> ✔ ended pipeline [234ms, 1 completed, 15 skipped] cached_sum #> [1] 385"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr-package.html","id":null,"dir":"Reference","previous_headings":"","what":"tarflowr: High-level Orchestration of 'targets' Pipelines — tarflowr-package","title":"tarflowr: High-level Orchestration of 'targets' Pipelines — tarflowr-package","text":"Provides simple, high-level interface creating executing 'targets'-based workflows, users can focus specific analysis, pipeline orchestration. philosophy tarflowr based concept arbitrary \"work units.\" package experimental.","code":""},{"path":[]},{"path":"http://humus.rocks/tarflowr/reference/tarflowr-package.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"tarflowr: High-level Orchestration of 'targets' Pipelines — tarflowr-package","text":"Maintainer: Andrew Brown brown.andrewg@gmail.com (ORCID)","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":null,"dir":"Reference","previous_headings":"","what":"Load a target from a tarflowr project into an environment. — tarflowr_load","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"simple wrapper around targets::tar_load(). function loads value target completed running tarflowr pipeline specified R environment.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"","code":"tarflowr_load(name, project_dir, envir = parent.frame())"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"name unquoted name target load. project_dir path tarflowr project directory containing _targets/objects folder load . envir environment load target . Defaults calling environment (parent.frame())","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"object name loaded specified environment envir side-effect.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_load.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Load a target from a tarflowr project into an environment. — tarflowr_load","text":"","code":"if (FALSE) { # \\dontrun{  td <- file.path(tempdir(), \"_my_project\")  res <- tarflowr_run(   work_units = list(a = 1, b = 2),   process_func = function(x) x * 10,   project_dir = td )  # load target object into current environment as side-effect tarflowr_load(work_seq, project_dir = td)  work_seq } # }"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":null,"dir":"Reference","previous_headings":"","what":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"work unit constructor designed hierarchical workflows. creates configuration object describes sub-project run single step within larger tarflowr meta-pipeline.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"","code":"tarflowr_project(project_dir, result_target, name = NULL)"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"project_dir path directory containing sub-project's _targets.R file. result_target unquoted name target within sub-project returned result work unit. name (Optional) short, descriptive name work unit, used naming branches main tarflowr pipeline. NULL, basename project_dir used.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"structured list object (tarflowr_work_unit) containing configuration sub-project.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"object created function intended passed element work_units list tarflowr_run(), tarflowr_run_subproject() set process_func.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_project.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Define a targets pipeline as a tarflowr work unit. — tarflowr_project","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":null,"dir":"Reference","previous_headings":"","what":"Read a target from a tarflowr project and return its value. — tarflowr_read","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"simple wrapper around targets::tar_read(). function reads value target completed running tarflowr pipeline returns object. generally preferred tarflowr_load() programmatic access.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"","code":"tarflowr_read(name, project_dir)"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"name unquoted name target read. project_dir path tarflowr project directory.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"value target.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_read.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Read a target from a tarflowr project and return its value. — tarflowr_read","text":"","code":"if (FALSE) { # \\dontrun{  td <- file.path(tempdir(), \"_my_project\")  tarflowr_run(   work_units = list(a = 1, b = 2),   process_func = function(x) x * 10,   combine_func = function(x) sum(unlist(x)),   project_dir = td )  # inspect the processed work units punits <- tarflowr_read(processed_unit, project_dir = td) punits  # inspect the final result res <- tarflowr_read(my_project_result, project_dir = td) res } # }"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":null,"dir":"Reference","previous_headings":"","what":"Restore a project's R environment using renv. — tarflowr_restore_env","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"function reads recorded package versions tarflowr project's metadata file (_tarflowr_meta.yaml) uses renv package install specific versions, recreating original R environment.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"","code":"tarflowr_restore_env(project_dir)"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"project_dir path tarflowr project directory.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"function designed make tarflowr projects highly reproducible. provides simple way another user (future self) ensure running code dependencies original author. prompt user initialize renv project one already active current directory. also checks current R version matches version recorded metadata issue warning differ.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_restore_env.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Restore a project's R environment using renv. — tarflowr_restore_env","text":"","code":"if (FALSE) { # \\dontrun{ # Assuming a project has already been run in \"./_my_project\"  # This will attempt to install the exact package versions # recorded in the project's metadata file. tarflowr_restore_env(project_dir = \"_my_project\") } # }"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"function serves high-level interface 'targets' 'crew' packages. programmatically generates _targets.R pipeline file based user-provided inputs, runs pipeline, returns final result. designed abstract complexity setting 'targets'-based workflow common \"map-reduce\" \"split-apply-combine\" tasks.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"","code":"tarflowr_run(   work_units,   process_func,   combine_func = NULL,   project_dir,   result_target_name = NULL,   packages = c(),   metadata = list(),   workers = 1L,   crew_controller = NULL,   seed = NULL,   error = \"stop\",   force = FALSE,   callr_function = callr::r,   callr_arguments = list(stdout = \"|\", stderr = \"2>&1\") )"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"work_units list vector. element represents single unit work processed. process_func function. Takes one element work_units first argument returns \"processed\" result. combine_func function. Takes list processed results process_func combines single, final object. NULL, final result list processed results. project_dir character. Path directory tarflowr project created. created exist. result_target_name character. Name last target pipeline, contains result evaluating combine_func list work units. NULL (default) final target name based project directory name, suffix \"_result\". packages character. Vector R package names required process_func combine_func. passed option packages targets::tar_option_set() loaded worker. metadata list. Named list elements write _tarflowr_meta.yaml file successful run. workers integer. Number local parallel workers use via crew package. used default crew_controller NULL. crew_controller crew_class_controller. Custom crew controller. Default NULL uses crew::crew_controller_local() specified number workers (parallel processes). targets::tar_make() called R option \"targets.controller set. seed integer. Random number seed. Passed targets::tar_option_set() via argument seed. error character. Error behavior. Either \"stop\" (default) \"continue\". Passed targets::tar_option_set() via argument error. force logical. FALSE (default), hash input arguments checked determine work units _targets.R file updated. TRUE new work units targets scripts written. callr_function function. Passed targets::tar_make(). Default callr::r() uses new R session. Use callr::r_bg() run background suppress targets pipeline output. callr_arguments list. Arguments passed targets::tar_make().","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"final combined result workflow, returned combine_func, list processed results combine_func NULL.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"function works creating self-contained project project_dir. serializes user's work_units functions (process_func, combine_func) directory. generates _targets.R script orchestrates following steps: Load work_units Map process_func element work_units using specified crew controller Combine results processing step using combine_func (optional) Execute pipeline targets::tar_make() Load final result original R session","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a parallelized, reproducible workflow using targets. — tarflowr_run","text":"","code":"if (FALSE) { # \\dontrun{  td <- file.path(tempdir(), \"_my_first_project\")  # define the work: a list of numbers my_work <- as.list(1:10)  # define the processing function to work on one item square_a_number <- function(x) {   Sys.sleep(1)   return(x^2) }  # define the combine function for the list of results sum_all_results <- function(results_list) {   sum(unlist(results_list)) }  # run the workflow final_sum <- tarflowr_run(   work_units = my_work,   process_func = square_a_number,   combine_func = sum_all_results,   project_dir = td,   workers = 4 )  # final target value is 385 print(final_sum)  # now inspect \"_my_first_project\" folder to see the # generated _targets.R file and the _targets/ cache  # rerun and get the result instantly from the cache: cached_sum <- tarflowr_run(   work_units = my_work,   process_func = square_a_number,   combine_func = sum_all_results,   project_dir = td,   workers = 4 ) print(cached_sum)  } # }"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":null,"dir":"Reference","previous_headings":"","what":"Run a targets sub-project — tarflowr_run_subproject","title":"Run a targets sub-project — tarflowr_run_subproject","text":"function designed used process_func tarflowr_run() call hierarchical workflows. takes configuration object tarflowr_project() executes specified targets pipeline.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"ref-usage","dir":"Reference","previous_headings":"","what":"Usage","title":"Run a targets sub-project — tarflowr_run_subproject","text":"","code":"tarflowr_run_subproject(config)"},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"arguments","dir":"Reference","previous_headings":"","what":"Arguments","title":"Run a targets sub-project — tarflowr_run_subproject","text":"config work unit object created tarflowr_project().","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"value","dir":"Reference","previous_headings":"","what":"Value","title":"Run a targets sub-project — tarflowr_run_subproject","text":"value result_target sub-project.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"details","dir":"Reference","previous_headings":"","what":"Details","title":"Run a targets sub-project — tarflowr_run_subproject","text":"function operates \"black box\" orchestrator sub-project. runs targets::tar_make() clean, separate R process using callr ensure parent child pipelines interfere one another. sub-project completes successfully, reads specified result target returns value.","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"author","dir":"Reference","previous_headings":"","what":"Author","title":"Run a targets sub-project — tarflowr_run_subproject","text":"Andrew G. Brown","code":""},{"path":"http://humus.rocks/tarflowr/reference/tarflowr_run_subproject.html","id":"ref-examples","dir":"Reference","previous_headings":"","what":"Examples","title":"Run a targets sub-project — tarflowr_run_subproject","text":"","code":"if (FALSE) { # \\dontrun{ # Example of a Hierarchical Workflow  # Assume you have two complex `targets` projects located at: #   - \"./study_site_A\" (which produces a target called `final_model_fit`) #   - \"./study_site_B\" (which also produces a target called `final_model_fit`)  # 1. Define the work units using the new helper function work_to_do <- list(   tarflowr_project(     project_dir = \"./study_site_A\",     result_target = final_model_fit,     name = \"site_A_run\"   ),   tarflowr_project(     project_dir = \"./study_site_B\",     result_target = final_model_fit,     name = \"site_B_run\"   ) )  # 2. Define a function to combine the final results combine_model_fits <- function(model_fit_list) {   names(model_fit_list) <- sapply(model_fit_list, function(x) x$site_name)   dplyr::bind_rows(model_fit_list, .id = \"source_project\") }  # 3. Run the meta-orchestrator all_fits <- tarflowr_run(   work_units = work_to_do,   process_func = run_targets_project, # Use the built-in function   combine_func = combine_model_fits,   project_dir = \"_meta_analysis_project\",   packages = c(\"dplyr\"), # Packages needed by combine_func   workers = 2 )  print(all_fits) } # }"},{"path":"http://humus.rocks/tarflowr/news/index.html","id":"tarflowr-001","dir":"Changelog","previous_headings":"","what":"tarflowr 0.0.1","title":"tarflowr 0.0.1","text":"Initial sketch package concept.","code":""}]
